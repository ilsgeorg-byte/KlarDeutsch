import os
import json
import time
import re
import psycopg2
from dotenv import load_dotenv
from groq import Groq

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ .env.local
dotenv_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env.local')
load_dotenv(dotenv_path)

# –î–æ—Å—Ç–∞–µ–º –∫–ª—é—á–∏
url = os.getenv("DATABASE_URL")
api_key = os.getenv("GROQ_API_KEY")

if not url or not api_key:
    print("‚ùå –û—à–∏–±–∫–∞: –£–±–µ–¥–∏—Å—å, —á—Ç–æ DATABASE_URL –∏ GROQ_API_KEY –µ—Å—Ç—å –≤ —Ñ–∞–π–ª–µ .env.local")
    exit(1)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç Groq
groq_client = Groq(api_key=api_key)

def get_linguistic_data_groq(de_word, level):
    prompt = f"""
    –í—ã–≤–µ–¥–∏ —Å—Ç—Ä–æ–≥–æ JSON –æ–±—ä–µ–∫—Ç –¥–ª—è –Ω–µ–º–µ—Ü–∫–æ–≥–æ —Å–ª–æ–≤–∞ "{de_word}" (—É—Ä–æ–≤–µ–Ω—å {level}).
    –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–∞–π —Ä—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞. –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ. –ù–∏–∫–∞–∫–∏—Ö –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤, —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç JSON:
    {{
      "ru_translation": "–¢–æ—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ (1-3 —Å–ª–æ–≤–∞)",
      "synonyms": "2 —Å–∏–Ω–æ–Ω–∏–º–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞",
      "antonyms": "1 –∞–Ω—Ç–æ–Ω–∏–º, –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞",
      "collocations": "2 –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è",
      "examples": [
        {{"de": "–ö–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–∏–º–µ—Ä 1.", "ru": "–ü–µ—Ä–µ–≤–æ–¥ 1."}},
        {{"de": "–ö–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–∏–º–µ—Ä 2.", "ru": "–ü–µ—Ä–µ–≤–æ–¥ 2."}},
        {{"de": "–ö–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–∏–º–µ—Ä 3.", "ru": "–ü–µ—Ä–µ–≤–æ–¥ 3."}}
      ]
    }}
    """
    
    try:
        completion = groq_client.chat.completions.create(
            # –ï—Å–ª–∏ Llama —Å–ø–æ—Ç—ã–∫–∞–µ—Ç—Å—è, –º–æ–∂–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å –Ω–∞ gemma2-9b-it
            model="llama-3.1-8b-instant", 
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            max_tokens=2000,
            response_format={"type": "json_object"}
        )
        
        text = completion.choices[0].message.content
        
        # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –≤—ã—Ä–µ–∑–∞–µ–º –≤—Å—ë –ª–∏—à–Ω–µ–µ –¥–æ –ø–µ—Ä–≤–æ–π –∏ –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ñ–∏–≥—É—Ä–Ω–æ–π —Å–∫–æ–±–∫–∏
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            text = match.group(0)
            
        return json.loads(text)
        
    except Exception as e:
        print(f"\n[!] –û—à–∏–±–∫–∞ API –¥–ª—è —Å–ª–æ–≤–∞ '{de_word}': {e}")
        return None

def update_database():
    print("–ü–æ–¥–∫–ª—é—á–∞—é—Å—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
    conn = None
    cur = None
    try:
        conn = psycopg2.connect(url)
        cur = conn.cursor()
        
        # –ò—â–µ–º —Å–ª–æ–≤–∞, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –ø—Ä–∏–º–µ—Ä–æ–≤ –ò–õ–ò –ø–µ—Ä–µ–≤–æ–¥ —Å—Ç–æ–∏—Ç –∫–∞–∫ "–ø–µ—Ä–µ–≤–æ–¥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ"
        cur.execute("""
            SELECT id, de, level 
            FROM words 
            WHERE examples IS NULL OR ru = '–ø–µ—Ä–µ–≤–æ–¥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ' 
            ORDER BY id ASC;
        """)
        words_to_update = cur.fetchall()
        
        total = len(words_to_update)
        print(f"–ù–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {total}\n")
        
        for i, row in enumerate(words_to_update, 1):
            word_id = row[0]
            de = row[1]
            level = row[2]
            
            print(f"[{i}/{total}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {de}...", end=" ", flush=True)
            
            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —É –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            data = get_linguistic_data_groq(de, level)
            
            if not data:
                print("‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ (–æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)")
                continue
                
            # –î–æ—Å—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON –æ—Ç–≤–µ—Ç–∞
            ru_translation = data.get("ru_translation", "–ü–µ—Ä–µ–≤–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            synonyms = data.get("synonyms", "")
            antonyms = data.get("antonyms", "")
            collocations = data.get("collocations", "")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–∞—Å—Å–∏–≤ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ JSON —Å—Ç—Ä–æ–∫—É –¥–ª—è –±–∞–∑—ã
            examples_list = data.get("examples", [])
            examples_json = json.dumps(examples_list, ensure_ascii=False)
            
            # –ß–∏—Å—Ç–∏–º –ø—Ä–æ–±–µ–ª—ã –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç—ã—Ö (—á—Ç–æ–±—ã –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–µ –Ω–µ —Å–ª–∏–ø–∞–ª–∏—Å—å)
            if synonyms: synonyms = ", ".join([s.strip() for s in synonyms.split(",") if s.strip()])
            if antonyms: antonyms = ", ".join([s.strip() for s in antonyms.split(",") if s.strip()])
            if collocations: collocations = ", ".join([c.strip() for c in collocations.split(",") if c.strip()])

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ, –≤–∫–ª—é—á–∞—è —Ä—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥
            cur.execute("""
                UPDATE words 
                SET ru = %s, synonyms = %s, antonyms = %s, collocations = %s, examples = %s::jsonb 
                WHERE id = %s;
            """, (ru_translation, synonyms, antonyms, collocations, examples_json, word_id))
            
            conn.commit()
            print("‚úÖ")
            
            # –ü–∞—É–∑–∞, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å –ª–∏–º–∏—Ç—ã API (Rate Limit)
            time.sleep(2)
            
        print("\nüéâ –í—Å–µ —Å–ª–æ–≤–∞ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã!")
            
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        if conn: conn.rollback()
    finally:
        if cur: cur.close()
        if conn:
            conn.close()
            print("–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –∑–∞–∫—Ä—ã—Ç–æ.")

if __name__ == "__main__":
    update_database()
