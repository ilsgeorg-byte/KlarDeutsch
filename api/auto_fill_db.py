import os
import json
import time
import re
import psycopg2
from dotenv import load_dotenv
from groq import Groq


dotenv_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env.local')
load_dotenv(dotenv_path)

url = os.getenv("DATABASE_URL")
api_key = os.getenv("GROQ_API_KEY")

if not url or not api_key:
    print("‚ùå –û—à–∏–±–∫–∞: –£–±–µ–¥–∏—Å—å, —á—Ç–æ DATABASE_URL –∏ GROQ_API_KEY –µ—Å—Ç—å –≤ —Ñ–∞–π–ª–µ .env.local")
    exit(1)

groq_client = Groq(api_key=api_key)

SEPARABLE_PREFIXES = [
    "ab", "an", "auf", "aus", "bei", "da", "durch", "ein", "fort",
    "her", "hin", "los", "mit", "nach", "vor", "weg", "weiter",
    "wieder", "zu", "zur√ºck", "zusammen"
]


def get_phrase_keywords(de_word):
    """
    –î–ª—è —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π —Ç–∏–ø–∞ 'gelten als', 'es gibt', 'Angst haben vor'
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –≤—Å–µ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –ø—Ä–∏–º–µ—Ä–µ.
    –î–ª—è –æ–±—ã—á–Ω—ã—Ö —Å–ª–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
    """
    parts = de_word.strip().lower().split()
    if len(parts) >= 2:
        stopwords = {"der", "die", "das", "sich", "den", "dem", "ein", "eine"}
        keywords = [p for p in parts if p not in stopwords]
        return keywords
    return []


def normalize(text):
    return (text.lower()
            .replace("√§", "a").replace("√∂", "o").replace("√º", "u")
            .replace("√ü", "ss"))


def has_cyrillic(text):
    if not text:
        return False
    return bool(re.search(r'[–∞-—è—ë–ê-–Ø–Å]', text))


def get_separable_parts(de_word):
    """
    –î–ª—è –æ—Ç–¥–µ–ª—è–µ–º—ã—Ö –≥–ª–∞–≥–æ–ª–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–ø—Ä–∏—Å—Ç–∞–≤–∫–∞, –æ—Å–Ω–æ–≤–∞).
    –ù–∞–ø—Ä–∏–º–µ—Ä: ausgehen ‚Üí ('aus', 'geh')
    –ï—Å–ª–∏ –Ω–µ –æ—Ç–¥–µ–ª—è–µ–º—ã–π ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (None, None).
    """
    word = de_word.strip()
    for prefix in ["der ", "die ", "das ", "sich ", "Den ", "Die ", "Das "]:
        if word.startswith(prefix):
            word = word[len(prefix):]
    word = word.split()[0].lower()
    word = word.replace("√§", "a").replace("√∂", "o").replace("√º", "u").replace("√ü", "ss")

    for prefix in SEPARABLE_PREFIXES:
        if word.startswith(prefix) and len(word) > len(prefix) + 2:
            stem = word[len(prefix):]
            if len(stem) > 4:
                stem = stem[:-2]
            return prefix, stem

    return None, None


def get_word_root(de_word):
    word = de_word.strip()
    for prefix in ["der ", "die ", "das ", "sich ", "Den ", "Die ", "Das "]:
        if word.startswith(prefix):
            word = word[len(prefix):]
    word = word.split()[0].lower()
    word = word.replace("√§", "a").replace("√∂", "o").replace("√º", "u").replace("√ü", "ss")
    if len(word) > 4:
        word = word[:-2]
    return word


def example_contains_word(example_de, de_word):
    example_norm = normalize(example_de)

    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π (gelten als, es gibt, Angst haben vor...)
    keywords = get_phrase_keywords(de_word)
    if keywords:
        norm_keywords = [normalize(k) for k in keywords]
        if all(kw in example_norm for kw in norm_keywords):
            return True

    # 2. –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ—Ä–Ω—é
    root = get_word_root(de_word)
    if root in example_norm:
        return True

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–¥–µ–ª—è–µ–º–æ–≥–æ –≥–ª–∞–≥–æ–ª–∞
    prefix, stem = get_separable_parts(de_word)
    if prefix and stem:
        if prefix in example_norm and stem in example_norm:
            return True

    return False


def example_is_german(example_de):
    return not has_cyrillic(example_de)


def examples_are_diverse(examples_list):
    if len(examples_list) < 2:
        return True
    texts = [normalize(ex.get("de", "")) for ex in examples_list]
    for i in range(len(texts)):
        for j in range(i + 1, len(texts)):
            words_i = set(texts[i].split())
            words_j = set(texts[j].split())
            if not words_i or not words_j:
                continue
            similarity = len(words_i & words_j) / min(len(words_i), len(words_j))
            if similarity > 0.6:
                return False
    return True


def examples_are_valid(examples_list, de_word):
    if not examples_list:
        return False
    for ex in examples_list:
        if isinstance(ex, dict) and "de" in ex:
            if not example_contains_word(ex["de"], de_word):
                return False
            if not example_is_german(ex["de"]):
                return False
    if not examples_are_diverse(examples_list):
        return False
    return True


def word_data_is_valid(de_word, ru, synonyms, antonyms, collocations, examples_list):
    if not ru or ru == "–ø–µ—Ä–µ–≤–æ–¥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ":
        return False
    if has_cyrillic(synonyms):
        return False
    if has_cyrillic(antonyms):
        return False
    if has_cyrillic(collocations):
        return False
    if not examples_list or len(examples_list) < 3:
        return False
    if not examples_are_valid(examples_list, de_word):
        return False
    return True


def get_linguistic_data_groq(de_word, level):
    prompt = f"""
–í—ã–≤–µ–¥–∏ —Å—Ç—Ä–æ–≥–æ JSON –æ–±—ä–µ–∫—Ç –¥–ª—è –Ω–µ–º–µ—Ü–∫–æ–≥–æ —Å–ª–æ–≤–∞ "{de_word}" (—É—Ä–æ–≤–µ–Ω—å {level}).
–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞:
1. –ö–∞–∂–¥—ã–π –∏–∑ 3 –ø—Ä–∏–º–µ—Ä–æ–≤ –û–ë–Ø–ó–ê–ù —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏–º–µ–Ω–Ω–æ —Å–ª–æ–≤–æ "{de_word}" (–∏–ª–∏ –µ–≥–æ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫—É—é —Ñ–æ—Ä–º—É).
2. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–∏–Ω–æ–Ω–∏–º—ã –∏–ª–∏ –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞ –≤–º–µ—Å—Ç–æ "{de_word}" –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö.
3. –ü–æ–ª–µ "de" –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö ‚Äî –¢–û–õ–¨–ö–û –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º —è–∑—ã–∫–µ. –ù–∏–∫–∞–∫–æ–≥–æ —Ä—É—Å—Å–∫–æ–≥–æ –≤ –ø–æ–ª–µ "de".
4. –ü–æ–ª–µ "ru" –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö ‚Äî –¢–û–õ–¨–ö–û –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
5. –í—Å–µ 3 –ø—Ä–∏–º–µ—Ä–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–∑–Ω—ã–º–∏ –ø–æ —Å–º—ã—Å–ª—É –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ.
6. synonyms, antonyms, collocations ‚Äî –¢–û–õ–¨–ö–û –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º —è–∑—ã–∫–µ, –Ω–∏–∫–∞–∫–æ–≥–æ —Ä—É—Å—Å–∫–æ–≥–æ.
7. –ü–µ—Ä–µ–≤–æ–¥ ‚Äî –∫—Ä–∞—Ç–∫–∏–π, 1-3 —Å–ª–æ–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.
8. –ù–∏–∫–∞–∫–∏—Ö –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤, —Ç–æ–ª—å–∫–æ JSON:

{{
  "ru_translation": "–¢–æ—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ (1-3 —Å–ª–æ–≤–∞)",
  "synonyms": "2 —Å–∏–Ω–æ–Ω–∏–º–∞ –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞",
  "antonyms": "1 –∞–Ω—Ç–æ–Ω–∏–º –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º, –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞",
  "collocations": "2 –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º —Å —Å–ª–æ–≤–æ–º {de_word}",
  "examples": [
    {{"de": "–ù–µ–º–µ—Ü–∫–∏–π –ø—Ä–∏–º–µ—Ä 1 —Å {de_word} ‚Äî –æ–¥–Ω–∞ —Å–∏—Ç—É–∞—Ü–∏—è.", "ru": "–†—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ 1."}},
    {{"de": "–ù–µ–º–µ—Ü–∫–∏–π –ø—Ä–∏–º–µ—Ä 2 —Å {de_word} ‚Äî –¥—Ä—É–≥–∞—è —Å–∏—Ç—É–∞—Ü–∏—è.", "ru": "–†—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ 2."}},
    {{"de": "–ù–µ–º–µ—Ü–∫–∏–π –ø—Ä–∏–º–µ—Ä 3 —Å {de_word} ‚Äî —Ç—Ä–µ—Ç—å—è —Å–∏—Ç—É–∞—Ü–∏—è.", "ru": "–†—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ 3."}}
  ]
}}
"""
    try:
        completion = groq_client.chat.completions.create(
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
            max_tokens=2000,
            response_format={"type": "json_object"}
        )
        text = completion.choices[0].message.content
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            text = match.group(0)
        return json.loads(text)

    except Exception as e:
        error_str = str(e)
        if "tokens per day" in error_str or "TPD" in error_str:
            print(f"\nüö´ –î–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å—á–µ—Ä–ø–∞–Ω. –ó–∞–ø—É—Å—Ç–∏ —Å–∫—Ä–∏–ø—Ç –∑–∞–≤—Ç—Ä–∞.")
            print(f"   –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ —Å–ª–æ–≤–µ: '{de_word}'")
            exit(0)
        if "429" in error_str:
            print(f"\n‚è≥ Rate limit, –∂–¥—É 60 —Å–µ–∫—É–Ω–¥...")
            time.sleep(60)
            return None
        print(f"\n[!] –û—à–∏–±–∫–∞ API –¥–ª—è —Å–ª–æ–≤–∞ '{de_word}': {e}")
        return None


def filter_valid_examples(examples_list, de_word):
    valid = []
    fallback = []

    for ex in examples_list:
        if isinstance(ex, dict) and "de" in ex and "ru" in ex:
            if not example_is_german(ex["de"]):
                print(f"\n  ‚ö†Ô∏è  –ü—Ä–∏–º–µ—Ä –æ—Ç–∫–ª–æ–Ω—ë–Ω (–Ω–µ –Ω–µ–º–µ—Ü–∫–∏–π): {ex['de']}")
                continue
            if not example_contains_word(ex["de"], de_word):
                print(f"\n  ‚ö†Ô∏è  –ü—Ä–∏–º–µ—Ä –æ—Ç–∫–ª–æ–Ω—ë–Ω (–Ω–µ—Ç —Å–ª–æ–≤–∞ '{de_word}'): {ex['de']}")
                fallback.append(ex)
                continue
            valid.append(ex)

    if not valid and fallback:
        print(f"\n  ‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è '{de_word}'")
        return fallback

    return valid


def update_database():
    print("–ü–æ–¥–∫–ª—é—á–∞—é—Å—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
    conn = None
    cur = None
    try:
        conn = psycopg2.connect(url)
        cur = conn.cursor()

        cur.execute("""
            SELECT id, de, level, examples, ru, synonyms, antonyms, collocations
            FROM words
            ORDER BY id ASC;
        """)
        all_words = cur.fetchall()

        total = len(all_words)
        print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤ –≤ –±–∞–∑–µ: {total}\n")

        needs_update = []

        print("–ü—Ä–æ–≤–µ—Ä—è—é –≤—Å–µ –ø–æ–ª—è –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å...")
        for row in all_words:
            word_id, de, level, examples_raw, ru, synonyms, antonyms, collocations = row

            # –ù–µ–∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ ‚Äî —Å—Ä–∞–∑—É –≤ –æ—á–µ—Ä–µ–¥—å
            if examples_raw is None or ru == "–ø–µ—Ä–µ–≤–æ–¥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ":
                needs_update.append(row)
                continue

            # –ó–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ ‚Äî –ø–æ–ª–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö –ø–æ–ª–µ–π
            try:
                examples_list = (examples_raw
                                 if isinstance(examples_raw, list)
                                 else json.loads(examples_raw))
                if not word_data_is_valid(de, ru, synonyms, antonyms,
                                          collocations, examples_list):
                    print(f"  ‚ö†Ô∏è  –¢—Ä–µ–±—É–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {de}")
                    needs_update.append(row)
            except Exception:
                needs_update.append(row)

        print(f"\n–ù–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è/–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {len(needs_update)}\n")

        for i, row in enumerate(needs_update, 1):
            word_id, de, level = row[0], row[1], row[2]

            print(f"[{i}/{len(needs_update)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {de}...", end=" ", flush=True)

            data = get_linguistic_data_groq(de, level)

            if not data:
                print("‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ (–æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)")
                continue

            ru_translation = data.get("ru_translation", "–ü–µ—Ä–µ–≤–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            synonyms = data.get("synonyms", "")
            antonyms = data.get("antonyms", "")
            collocations = data.get("collocations", "")

            if has_cyrillic(synonyms):
                print(f"\n  ‚ö†Ô∏è  –°–∏–Ω–æ–Ω–∏–º—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –æ—á–∏—â–∞—é: {synonyms}")
                synonyms = ""
            if has_cyrillic(antonyms):
                print(f"\n  ‚ö†Ô∏è  –ê–Ω—Ç–æ–Ω–∏–º –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –æ—á–∏—â–∞—é: {antonyms}")
                antonyms = ""
            if has_cyrillic(collocations):
                print(f"\n  ‚ö†Ô∏è  –°–≤—è–∑–∫–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –æ—á–∏—â–∞—é: {collocations}")
                collocations = ""

            examples_list = data.get("examples", [])
            valid_examples = filter_valid_examples(examples_list, de)

            if not valid_examples:
                print(f"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è '{de}')")
                continue

            if len(valid_examples) < 3:
                print(f"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(valid_examples)} –∏–∑ 3)")
                continue

            if not examples_are_diverse(valid_examples):
                print(f"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ (–ø—Ä–∏–º–µ—Ä—ã —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏ –¥–ª—è '{de}')")
                continue

            examples_json = json.dumps(valid_examples, ensure_ascii=False)

            if synonyms:
                synonyms = ", ".join([s.strip() for s in synonyms.split(",") if s.strip()])
            if antonyms:
                antonyms = ", ".join([s.strip() for s in antonyms.split(",") if s.strip()])
            if collocations:
                collocations = ", ".join([c.strip() for c in collocations.split(",") if c.strip()])

            cur.execute("""
                UPDATE words
                SET ru = %s, synonyms = %s, antonyms = %s, collocations = %s, examples = %s::jsonb
                WHERE id = %s;
            """, (ru_translation, synonyms, antonyms, collocations, examples_json, word_id))

            conn.commit()
            print("‚úÖ")

            time.sleep(2)

        print("\nüéâ –ì–æ—Ç–æ–≤–æ! –í—Å–µ —Å–ª–æ–≤–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã.")

    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        if conn:
            conn.rollback()
    finally:
        if cur:
            cur.close()
        if conn:
            conn.close()
            print("–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –∑–∞–∫—Ä—ã—Ç–æ.")


if __name__ == "__main__":
    update_database()

