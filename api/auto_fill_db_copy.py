import os
import json
import time
import re
import psycopg2
from dotenv import load_dotenv
from groq import Groq


dotenv_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env.local')
load_dotenv(dotenv_path)

url = os.getenv("DATABASE_URL")
api_key = os.getenv("GROQ_API_KEY")

if not url or not api_key:
    print("‚ùå –û—à–∏–±–∫–∞: –£–±–µ–¥–∏—Å—å, —á—Ç–æ DATABASE_URL –∏ GROQ_API_KEY –µ—Å—Ç—å –≤ —Ñ–∞–π–ª–µ .env.local")
    exit(1)

groq_client = Groq(api_key=api_key)


def get_word_root(de_word):
    """–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ—Ä–µ–Ω—å —Å–ª–æ–≤–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–º–ª–∞—É—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞."""
    word = de_word.strip()

    # –£–±–∏—Ä–∞–µ–º –∞—Ä—Ç–∏–∫–ª–∏ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —á–∞—Å—Ç–∏
    for prefix in ["der ", "die ", "das ", "sich ", "Den ", "Die ", "Das "]:
        if word.startswith(prefix):
            word = word[len(prefix):]

    # –ë–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –µ—Å–ª–∏ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä "ab und zu" ‚Üí "ab")
    word = word.split()[0].lower()

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–º–ª–∞—É—Ç—ã ‚Äî –∏—â–µ–º –∏ —Å —É–º–ª–∞—É—Ç–æ–º –∏ –±–µ–∑
    word = word.replace("√§", "a").replace("√∂", "o").replace("√º", "u")

    return word

def normalize(text):
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç –ø—Ä–∏–º–µ—Ä–∞ —Ç–∞–∫ –∂–µ ‚Äî —É–±–∏—Ä–∞–µ–º —É–º–ª–∞—É—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
    return (text.lower()
            .replace("√§", "a").replace("√∂", "o").replace("√º", "u")
            .replace("√ü", "ss"))



def example_contains_word(example_de, de_word):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ —Å —É—á—ë—Ç–æ–º —É–º–ª–∞—É—Ç–æ–≤ –∏ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º."""
    root = get_word_root(de_word)
    example_normalized = normalize(example_de)
    return root in example_normalized


def get_linguistic_data_groq(de_word, level):
    prompt = f"""
–í—ã–≤–µ–¥–∏ —Å—Ç—Ä–æ–≥–æ JSON –æ–±—ä–µ–∫—Ç –¥–ª—è –Ω–µ–º–µ—Ü–∫–æ–≥–æ —Å–ª–æ–≤–∞ "{de_word}" (—É—Ä–æ–≤–µ–Ω—å {level}).
–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞:
1. –ö–∞–∂–¥—ã–π –∏–∑ 3 –ø—Ä–∏–º–µ—Ä–æ–≤ –û–ë–Ø–ó–ê–ù —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏–º–µ–Ω–Ω–æ —Å–ª–æ–≤–æ "{de_word}" (–∏–ª–∏ –µ–≥–æ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫—É—é —Ñ–æ—Ä–º—É).
2. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–∏–Ω–æ–Ω–∏–º—ã –∏–ª–∏ –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞ –≤–º–µ—Å—Ç–æ "{de_word}" –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö.
3. –ü–µ—Ä–µ–≤–æ–¥ ‚Äî –∫—Ä–∞—Ç–∫–∏–π, 1-3 —Å–ª–æ–≤–∞.
4. –ù–∏–∫–∞–∫–∏—Ö –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤, —Ç–æ–ª—å–∫–æ JSON:

{{
  "ru_translation": "–¢–æ—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ (1-3 —Å–ª–æ–≤–∞)",
  "synonyms": "2 —Å–∏–Ω–æ–Ω–∏–º–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞",
  "antonyms": "1 –∞–Ω—Ç–æ–Ω–∏–º, –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞",
  "collocations": "2 –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è —Å —Å–ª–æ–≤–æ–º {de_word}",
  "examples": [
    {{"de": "–ü—Ä–∏–º–µ—Ä 1 —Å —Å–ª–æ–≤–æ–º {de_word} –∏–ª–∏ –µ–≥–æ —Ñ–æ—Ä–º–æ–π.", "ru": "–ü–µ—Ä–µ–≤–æ–¥ 1."}},
    {{"de": "–ü—Ä–∏–º–µ—Ä 2 —Å —Å–ª–æ–≤–æ–º {de_word} –∏–ª–∏ –µ–≥–æ —Ñ–æ—Ä–º–æ–π.", "ru": "–ü–µ—Ä–µ–≤–æ–¥ 2."}},
    {{"de": "–ü—Ä–∏–º–µ—Ä 3 —Å —Å–ª–æ–≤–æ–º {de_word} –∏–ª–∏ –µ–≥–æ —Ñ–æ—Ä–º–æ–π.", "ru": "–ü–µ—Ä–µ–≤–æ–¥ 3."}}
  ]
}}
"""

    try:
        completion = groq_client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            max_tokens=2000,
            response_format={"type": "json_object"}
        )

        text = completion.choices[0].message.content

        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            text = match.group(0)

        return json.loads(text)

    except Exception as e:
        print(f"\n[!] –û—à–∏–±–∫–∞ API –¥–ª—è —Å–ª–æ–≤–∞ '{de_word}': {e}")
        return None


def filter_valid_examples(examples_list, de_word):
    """–û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä—ã, –≥–¥–µ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å —Ü–µ–ª–µ–≤–æ–µ —Å–ª–æ–≤–æ."""
    valid = []
    for ex in examples_list:
        if isinstance(ex, dict) and "de" in ex and "ru" in ex:
            if example_contains_word(ex["de"], de_word):
                valid.append(ex)
            else:
                print(f"\n  ‚ö†Ô∏è  –ü—Ä–∏–º–µ—Ä –æ—Ç–∫–ª–æ–Ω—ë–Ω (–Ω–µ—Ç —Å–ª–æ–≤–∞ '{de_word}'): {ex['de']}")
    return valid


def update_database():
    print("–ü–æ–¥–∫–ª—é—á–∞—é—Å—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
    conn = None
    cur = None
    try:
        conn = psycopg2.connect(url)
        cur = conn.cursor()

        cur.execute("""
            SELECT id, de, level 
            FROM words 
            WHERE examples IS NULL OR ru = '–ø–µ—Ä–µ–≤–æ–¥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ' 
            ORDER BY id ASC;
        """)
        words_to_update = cur.fetchall()

        total = len(words_to_update)
        print(f"–ù–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {total}\n")

        for i, row in enumerate(words_to_update, 1):
            word_id = row[0]
            de = row[1]
            level = row[2]

            print(f"[{i}/{total}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {de}...", end=" ", flush=True)

            data = get_linguistic_data_groq(de, level)

            if not data:
                print("‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ (–æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)")
                continue

            ru_translation = data.get("ru_translation", "–ü–µ—Ä–µ–≤–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            synonyms = data.get("synonyms", "")
            antonyms = data.get("antonyms", "")
            collocations = data.get("collocations", "")

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã ‚Äî —Ç–æ–ª—å–∫–æ —Ç–µ, –≥–¥–µ –µ—Å—Ç—å —Ü–µ–ª–µ–≤–æ–µ —Å–ª–æ–≤–æ
            examples_list = data.get("examples", [])
            valid_examples = filter_valid_examples(examples_list, de)

            # –ï—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –Ω–µ –ø—Ä–æ—à—ë–ª —Ñ–∏–ª—å—Ç—Ä ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–æ–≤–æ
            if not valid_examples:
                print(f"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–∏ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–≤–æ '{de}')")
                continue

            examples_json = json.dumps(valid_examples, ensure_ascii=False)

            if synonyms:
                synonyms = ", ".join([s.strip() for s in synonyms.split(",") if s.strip()])
            if antonyms:
                antonyms = ", ".join([s.strip() for s in antonyms.split(",") if s.strip()])
            if collocations:
                collocations = ", ".join([c.strip() for c in collocations.split(",") if c.strip()])

            cur.execute("""
                UPDATE words 
                SET ru = %s, synonyms = %s, antonyms = %s, collocations = %s, examples = %s::jsonb 
                WHERE id = %s;
            """, (ru_translation, synonyms, antonyms, collocations, examples_json, word_id))

            conn.commit()
            print("‚úÖ")

            time.sleep(2)

        print("\nüéâ –í—Å–µ —Å–ª–æ–≤–∞ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã!")

    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        if conn:
            conn.rollback()
    finally:
        if cur:
            cur.close()
        if conn:
            conn.close()
            print("–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –∑–∞–∫—Ä—ã—Ç–æ.")


if __name__ == "__main__":
    update_database()
