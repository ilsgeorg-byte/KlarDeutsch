import os
import json
import time
import re
import psycopg2
import traceback
from dotenv import load_dotenv
from groq import Groq


dotenv_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env.local')
load_dotenv(dotenv_path)

url = os.getenv("DATABASE_URL")
api_key = os.getenv("GROQ_API_KEY")

if not url or not api_key:
    print("‚ùå –û—à–∏–±–∫–∞: –£–±–µ–¥–∏—Å—å, —á—Ç–æ DATABASE_URL –∏ GROQ_API_KEY –µ—Å—Ç—å –≤ —Ñ–∞–π–ª–µ .env.local")
    exit(1)

groq_client = Groq(api_key=api_key)

IRREGULAR_FORMS = {
    "sein":     ["bin", "ist", "war", "sind", "seid", "gewes"],
    "haben":    ["hat", "hab", "hatt", "h√§tt"],
    "werden":   ["wird", "wurde", "w√ºrd", "worden"],
    "k√∂nnen":   ["kann", "konn", "k√∂nn", "konnt"],
    "d√ºrfen":   ["darf", "durf", "d√ºrf", "durft"],
    "m√ºssen":   ["muss", "m√ºss", "musst"],
    "wollen":   ["will", "woll", "wollt"],
    "sollen":   ["soll", "sollt"],
    "m√∂gen":    ["mag", "mog", "m√∂ch", "mocht"],
    "wissen":   ["wei√ü", "wuss", "wiss"],
    "sehen":    ["sieht", "sieh", "sah"],
    "gehen":    ["geht", "ging"],
    "kommen":   ["kommt", "kam"],
    "nehmen":   ["nimmt", "nahm"],
    "geben":    ["gibt", "gab"],
    "lassen":   ["l√§sst", "lass", "lie√ü"],
    "fahren":   ["f√§hrt", "fuhr"],
    "laufen":   ["l√§uft", "lief"],
    "essen":    ["isst", "a√ü"],
    "lesen":    ["liest", "las"],
    "gelten":   ["gilt", "galt"],
    "erweisen": ["erwies", "erwiesen"],
    "stehen":   ["steht", "stand", "standen"],
    "halten":   ["h√§lt", "hielt"],
    "fallen":   ["f√§llt", "fiel"],
    "treffen":  ["trifft", "traf"],
    "helfen":   ["hilft", "half"],
    "finden":   ["findet", "fand"],
    "bringen":  ["bringt", "brachte"],
    "denken":   ["denkt", "dachte"],
    "sprechen": ["spricht", "sprach"],
    "schreiben":["schreibt", "schrieb"],
}

SEPARABLE_PREFIXES = [
    "ab", "an", "auf", "aus", "bei", "da", "durch", "ein", "fort",
    "her", "hin", "los", "mit", "nach", "vor", "weg", "weiter",
    "wieder", "zu", "zur√ºck", "zusammen"
]


def normalize(text):
    return (text.lower()
            .replace("√§", "a").replace("√∂", "o").replace("√º", "u")
            .replace("√ü", "ss"))


def has_cyrillic(text):
    if not text:
        return False
    return bool(re.search(r'[–∞-—è—ë–ê-–Ø–Å]', text))


def get_separable_parts(de_word):
    word = de_word.strip()
    for prefix in ["der ", "die ", "das ", "sich "]:
        if word.lower().startswith(prefix):
            word = word[len(prefix):]
    word = word.split()[0].lower()
    word = normalize(word)
    for prefix in SEPARABLE_PREFIXES:
        if word.startswith(prefix) and len(word) > len(prefix) + 2:
            stem = word[len(prefix):]
            if len(stem) > 4:
                stem = stem[:-2]
            return prefix, stem
    return None, None


def get_word_root(de_word):
    word = de_word.strip()
    for prefix in ["der ", "die ", "das ", "sich "]:
        if word.lower().startswith(prefix):
            word = word[len(prefix):]
    word = word.split()[0].lower()
    word = normalize(word)
    if len(word) > 4:
        word = word[:-2]
    return word


def example_contains_word(example_de, de_word):
    def norm(text):
        return text.lower().replace("√§","a").replace("√∂","o").replace("√º","u").replace("√ü","ss")

    example_lower = example_de.lower()
    word_lower = de_word.lower()
    example_norm = norm(example_de)

    # 1. –ü—Ä—è–º–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
    if word_lower in example_lower:
        return True

    # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–ª–∞–≤–Ω—ã–π –≥–ª–∞–≥–æ–ª (—É–±–∏—Ä–∞–µ–º sich, als, zu)
    stop = {"sich", "als", "zu", "nicht"}
    parts = [p for p in word_lower.split() if p not in stop]
    main_verb = parts[0] if parts else word_lower

    # 3. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã
    if main_verb in IRREGULAR_FORMS:
        for form in IRREGULAR_FORMS[main_verb]:
            if norm(form) in example_norm:
                return True

    # 4. –û—Ç–¥–µ–ª—è–µ–º—ã–µ –≥–ª–∞–≥–æ–ª—ã (aussehen ‚Üí sieht...aus)
    prefix, stem = get_separable_parts(main_verb)
    if prefix and stem:
        for inf, forms in IRREGULAR_FORMS.items():
            if norm(inf).startswith(stem):
                if prefix in example_lower:
                    for form in forms:
                        if norm(form) in example_norm:
                            return True
        if stem in example_norm and prefix in example_lower:
            return True

    # 5. –°—Ç–µ–º–º–∏–Ω–≥ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –≥–ª–∞–≥–æ–ª–æ–≤
    root = get_word_root(main_verb)
    if len(root) >= 4 and root in example_norm:
        return True

    return False


def example_is_german(example_de):
    return not has_cyrillic(example_de)


def examples_are_diverse(examples_list):
    if len(examples_list) < 2:
        return True
    texts = [normalize(ex.get("de", "")) for ex in examples_list]
    for i in range(len(texts)):
        for j in range(i + 1, len(texts)):
            words_i = set(texts[i].split())
            words_j = set(texts[j].split())
            if not words_i or not words_j:
                continue
            similarity = len(words_i & words_j) / min(len(words_i), len(words_j))
            if similarity > 0.6:
                return False
    return True


def filter_valid_examples(examples_list, de_word):
    valid = []
    fallback = []
    for ex in examples_list:
        if isinstance(ex, dict) and "de" in ex and "ru" in ex:
            if not example_is_german(ex["de"]):
                continue
            if not example_contains_word(ex["de"], de_word):
                print(f"\n  ‚ö†Ô∏è  –ü—Ä–∏–º–µ—Ä –æ—Ç–∫–ª–æ–Ω—ë–Ω (–Ω–µ—Ç —Å–ª–æ–≤–∞ '{de_word}'): {ex['de']}")
                fallback.append(ex)
                continue
            valid.append(ex)
    if not valid and fallback:
        print(f"\n  ‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è '{de_word}'")
        return fallback
    return valid


def word_data_is_valid(de_word, ru, synonyms, antonyms, collocations, examples_list):
    if not ru or ru == "–ø–µ—Ä–µ–≤–æ–¥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ":
        return False
    if has_cyrillic(synonyms):
        return False
    if has_cyrillic(antonyms):
        return False
    if has_cyrillic(collocations):
        return False
    if not examples_list or len(examples_list) < 3:
        return False
    if not examples_are_diverse(examples_list):
        return False
    for ex in examples_list:
        if isinstance(ex, dict) and "de" in ex:
            if not example_contains_word(ex["de"], de_word):
                return False
            if not example_is_german(ex["de"]):
                return False
    return True


def get_linguistic_data_groq(de_word, level):
    prompt = f"""–í—ã–≤–µ–¥–∏ —Å—Ç—Ä–æ–≥–æ JSON –æ–±—ä–µ–∫—Ç –¥–ª—è –Ω–µ–º–µ—Ü–∫–æ–≥–æ —Å–ª–æ–≤–∞ "{de_word}" (—É—Ä–æ–≤–µ–Ω—å {level}).
–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞:
1. –ö–∞–∂–¥—ã–π –∏–∑ 3 –ø—Ä–∏–º–µ—Ä–æ–≤ –û–ë–Ø–ó–ê–ù —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏–º–µ–Ω–Ω–æ —Å–ª–æ–≤–æ "{de_word}" (–∏–ª–∏ –µ–≥–æ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫—É—é —Ñ–æ—Ä–º—É).
2. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–∏–Ω–æ–Ω–∏–º—ã –∏–ª–∏ –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞ –≤–º–µ—Å—Ç–æ "{de_word}" –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö.
3. –ü–æ–ª–µ "de" –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö ‚Äî –¢–û–õ–¨–ö–û –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º —è–∑—ã–∫–µ. –ù–∏–∫–∞–∫–æ–≥–æ —Ä—É—Å—Å–∫–æ–≥–æ –≤ –ø–æ–ª–µ "de".
4. –ü–æ–ª–µ "ru" –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö ‚Äî –¢–û–õ–¨–ö–û –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
5. –í—Å–µ 3 –ø—Ä–∏–º–µ—Ä–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–∑–Ω—ã–º–∏ –ø–æ —Å–º—ã—Å–ª—É –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ.
6. synonyms, antonyms, collocations ‚Äî –¢–û–õ–¨–ö–û –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º —è–∑—ã–∫–µ, –Ω–∏–∫–∞–∫–æ–≥–æ —Ä—É—Å—Å–∫–æ–≥–æ.
7. –ü–µ—Ä–µ–≤–æ–¥ ‚Äî –∫—Ä–∞—Ç–∫–∏–π, 1-3 —Å–ª–æ–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.
8. –ù–∏–∫–∞–∫–∏—Ö –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤, —Ç–æ–ª—å–∫–æ JSON:

{{
  "ru_translation": "–¢–æ—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ (1-3 —Å–ª–æ–≤–∞)",
  "synonyms": "2 —Å–∏–Ω–æ–Ω–∏–º–∞ –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞",
  "antonyms": "1 –∞–Ω—Ç–æ–Ω–∏–º –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º, –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞",
  "collocations": "2 –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º —Å —Å–ª–æ–≤–æ–º {de_word}",
  "examples": [
    {{"de": "–ù–µ–º–µ—Ü–∫–∏–π –ø—Ä–∏–º–µ—Ä 1 —Å {de_word}.", "ru": "–†—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ 1."}},
    {{"de": "–ù–µ–º–µ—Ü–∫–∏–π –ø—Ä–∏–º–µ—Ä 2 —Å {de_word}.", "ru": "–†—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ 2."}},
    {{"de": "–ù–µ–º–µ—Ü–∫–∏–π –ø—Ä–∏–º–µ—Ä 3 —Å {de_word}.", "ru": "–†—É—Å—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ 3."}}
  ]
}}"""
    try:
        completion = groq_client.chat.completions.create(
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
            max_tokens=2000,
            response_format={"type": "json_object"}
        )
        text = completion.choices[0].message.content
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            text = match.group(0)
        return json.loads(text)
    except Exception as e:
        error_str = str(e)
        if "tokens per day" in error_str or "TPD" in error_str:
            print("\nüö´ –î–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å—á–µ—Ä–ø–∞–Ω.")
            exit(0)
        if "429" in error_str:
            time.sleep(60)
            return None
        return None


def update_database():
    conn = psycopg2.connect(url)
    cur = conn.cursor()

    cur.execute("SELECT id, de, level, examples, ru, synonyms, antonyms, collocations FROM words ORDER BY id ASC;")
    all_words = cur.fetchall()

    needs_update = []
    for row in all_words:
        word_id, de, level, examples_raw, ru, synonyms, antonyms, collocations = row
        if examples_raw is None or ru == "–ø–µ—Ä–µ–≤–æ–¥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ":
            needs_update.append(row)
            continue
        try:
            examples_list = examples_raw if isinstance(examples_raw, list) else json.loads(examples_raw)
            if not word_data_is_valid(de, ru, synonyms, antonyms, collocations, examples_list):
                needs_update.append(row)
        except Exception:
            needs_update.append(row)

    print(f"–ù–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(needs_update)}")

    for i, row in enumerate(needs_update, 1):
        word_id, de, level, examples_raw, ru, synonyms, antonyms, collocations = row
        print(f"[{i}/{len(needs_update)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {de}... ", end="", flush=True)

        try:
            data = get_linguistic_data_groq(de, level)
            if not data:
                print("‚ùå (–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ—Ç API)")
                continue

            ru_translation = data.get("ru_translation", "–ü–µ—Ä–µ–≤–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            synonyms       = data.get("synonyms", "")
            antonyms       = data.get("antonyms", "")
            collocations   = data.get("collocations", "")
            examples_list  = data.get("examples", [])

            valid_examples = filter_valid_examples(examples_list, de)

            if len(valid_examples) < 3 or not examples_are_diverse(valid_examples):
                print(f"‚ùå (–ø—Ä–∏–º–µ—Ä–æ–≤: {len(valid_examples)}/3)")
                continue

            examples_json = json.dumps(valid_examples, ensure_ascii=False)
            cur.execute(
                "UPDATE words SET ru=%s, synonyms=%s, antonyms=%s, collocations=%s, examples=%s::jsonb WHERE id=%s;",
                (ru_translation, synonyms, antonyms, collocations, examples_json, word_id)
            )
            conn.commit()
            print("‚úÖ")
            time.sleep(2)

        except Exception as e:
            print(f"‚ùå {type(e).__name__}: {e}")
            traceback.print_exc()
            with open("update_words_errors.log", "a", encoding="utf-8") as f:
                f.write(f"{de}\t{type(e).__name__}: {e}\n")
            continue

    cur.close()
    conn.close()
    print("\nüéâ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    update_database()
